<h1 align="center">Gesture Based UI Development</h1>


<a href="https://www.gmit.ie/" >
<p align="center"><img src="https://i.ibb.co/f1ZQSkt/logo-gmit.png"
alt="GMIT Logo" width="500" height="200"/>
</p></a>


<p align="center">John Shields - G00348436</p>

***

<p align="center"><img src="https://user-images.githubusercontent.com/26766163/115625698-58c5a580-a2f4-11eb-9470-7b4db0bfa62a.png"
alt="eithne_logo" width="200"/></p>

# Overview
Originally for this project, a Virtual Reality detective game was the first choice.
This game would have been developed with Unity and the Oculus Quest.
Unfortunately, due to PC hardware limitations, it was not feasible to develop a sufficient game.
Many attempts were made to get the game set up, but even basic setups took hours. 
Being a hectic time during the college year, this could not carry on. Time is precious; therefore, improvisation had to be made.
The project's goal was then altered to be a Voice Assistant in Python with skills enhanced by AI technologies. 

# Purpose of the Application
***Design of the application including the screens of the user interface and
how it works. The application can be an experimentation process for you, testing how pieces of
hardware could interact or be combined with gestures.***


The application designed is a Voice Assistant by the name of Eithne.

Eithne is programmed to do the following features: 

* Google Search
* Google Maps
* Wikipedia Summaries
* YouTube Queries
* Open any Website with a ``dot com``
* Historical Events that happened Today from [numbersapi.com/day/month/date](http://numbersapi.com/04/6/date)


# Gestures of this application
***Consider how gestures can be incorporated
into the application, providing a justification for the ones that you pick. This is an important
research element for the project and needs to explain how the gestures fit into the solution you are
creating.***


# Hardware used in creating the application
***You are not limited to the hardware listed above. If
you have your own hardware, or hardware simulator that you wish to use, then feel free. The
purpose of each piece of hardware should be given with a comparison to other options available.***

# Architecture for the solution 
***The full architecture for the solution, including the class diagrams,
any data models, communications and distributed elements that you are creating. The architecture
must make sense when the gestures and the hardware are combined. Justification is necessary in
the documentation for this. You need to include a list of relevant libraries that you used in the
project.***

# Conclusions & Recommendations
***Conclusions are what you have learned from this project and
the associated research. Recommendations are what you would do differently if you were to
undertake the project again. The Reflective Piece – what I learned and “enjoyed”! This gives scope
for a critical evaluation of the project and the objective that you tried to achieve.***


# References

* [Build A Python Speech Assistant App](https://youtu.be/x8xjj6cR9Nc)
* [How to build your own AI personal assistant using Python](https://bit.ly/3auyANP)
* [The Ultimate Guide To Speech Recognition With Python](https://realpython.com/python-speech-recognition/)
* [ChatterBot](https://chatterbot.readthedocs.io/en/stable/index.html#)